{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import libraries and load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55Bc2TGW8J-h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score, adjusted_rand_score\n",
        "from collections import Counter\n",
        "\n",
        "# Load model directly\n",
        "from transformers import AutoImageProcessor, AutoModelForVideoClassification\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k600\", return_dict = True, output_hidden_states = True)\n",
        "model = AutoModelForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k600\", return_dict = True,  output_hidden_states = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gm0EOcF8zER"
      },
      "outputs": [],
      "source": [
        "# Function to extract frames from video with dynamic large step rate\n",
        "def extract_frames(video_path, small_step_rate=1, max_large_frames=8, max_small_frames=8):\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Calculate the total number of frames in the video\n",
        "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate dynamic large step rate\n",
        "    large_step_rate = max(1, total_frames // max_large_frames)\n",
        "\n",
        "    large_step_frames = []\n",
        "    small_step_frames = []\n",
        "    success, image = vidcap.read()\n",
        "    count = 0\n",
        "\n",
        "    while success and (len(large_step_frames) < max_large_frames or len(small_step_frames) < max_small_frames):\n",
        "        if count % large_step_rate == 0 and len(large_step_frames) < max_large_frames:\n",
        "            large_step_frames.append(image)\n",
        "        if count % small_step_rate == 0 and len(small_step_frames) < max_small_frames:\n",
        "            small_step_frames.append(image)\n",
        "        success, image = vidcap.read()\n",
        "        count += 1\n",
        "\n",
        "    vidcap.release()\n",
        "    return large_step_frames, small_step_frames\n",
        "\n",
        "\n",
        "# Function to embed video frames into vectors with max pooling\n",
        "def embed_video(frames):\n",
        "\n",
        "    inputs = processor(frames, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Use hidden states as embeddings\n",
        "    last_layer = outputs.hidden_states[-1]\n",
        "\n",
        "    # Apply max pooling across the logits\n",
        "    pooled_embeddings, _ = torch.max(last_layer, dim=1)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "def process_videos_in_folders(folders, labels):\n",
        "    embedded_vectors = []\n",
        "    embedded_labels = []\n",
        "\n",
        "    for folder, label in zip(folders, labels):\n",
        "        video_files = [f for f in os.listdir(folder) if f.endswith(\".mp4\")]\n",
        "\n",
        "        for video_file in video_files:\n",
        "            video_path = os.path.join(folder, video_file)\n",
        "            large_step_frames, small_step_frames = extract_frames(video_path)\n",
        "            embedding_vector_large = embed_video(large_step_frames)\n",
        "            embedding_vector_small = embed_video(small_step_frames)\n",
        "            embedding_vector = torch.cat((embedding_vector_large, embedding_vector_small), dim=0)\n",
        "            # Convert to numpy array and store in list\n",
        "            embedding_vector_np = embedding_vector.numpy()\n",
        "            embedded_vectors.append(embedding_vector_np)\n",
        "            embedded_labels.append(label)\n",
        "\n",
        "    return np.array(embedded_vectors), np.array(embedded_labels)\n",
        "\n",
        "# Function to perform KMeans clustering and compare with true labels\n",
        "def compare_kmeans_with_labels(embedded_vectors, true_labels, n_clusters=3):\n",
        "    # Perform KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    predicted_labels = kmeans.fit_predict(embedded_vectors)\n",
        "\n",
        "    # Since KMeans labels are arbitrary, we need to map them to the true labels\n",
        "    label_map = {}\n",
        "    for i in range(n_clusters):\n",
        "        # Find the most common true label in each cluster\n",
        "        cluster_indices = np.where(predicted_labels == i)[0]\n",
        "        true_labels_in_cluster = true_labels[cluster_indices]\n",
        "        most_common_label = Counter(true_labels_in_cluster).most_common(1)[0][0]\n",
        "        label_map[i] = most_common_label\n",
        "\n",
        "    # Map predicted labels to the most common true label in each cluster\n",
        "    mapped_labels = np.array([label_map[label] for label in predicted_labels])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "    # Calculate Adjusted Rand Index (a more robust clustering metric)\n",
        "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
        "\n",
        "    return accuracy, ari, mapped_labels, predicted_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq3vXGuO96Qa",
        "outputId": "d6afa1b8-e85c-4cb4-ac53-313d3c6e70c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example usage\n",
        "folders = [\"/content/drive/MyDrive/processed_glasses\",\n",
        "           \"/content/drive/MyDrive/processed_climb\",\n",
        "           \"/content/drive/MyDrive/processed_brush\"]\n",
        "\n",
        "labels = [0, 1, 2]  # Labels corresponding to the folders\n",
        "\n",
        "embedded_vectors, embedded_labels = process_videos_in_folders(folders, labels)\n",
        "# Reshape embedded_vectors to be 2D array\n",
        "embeddings = [np.concatenate((vec[0], vec[1])) for vec in embedded_vectors]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_thd86OHjOR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(embeddings)\n",
        "df['label'] = embedded_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a7GpTI7H0ui"
      },
      "outputs": [],
      "source": [
        "df.to_csv('../embeddings.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "accuracy, ari, mapped_labels, predicted_labels = compare_kmeans_with_labels(embeddings, embedded_labels, n_clusters=3)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Adjusted Rand Index (ARI): {ari}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
